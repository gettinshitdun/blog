{"index":{"slug":"index","filePath":"index.md","title":"Home","links":["Kotia/Kotia's-Home"],"tags":[],"content":"Who are we?\nA couple of people fresh into the industry with some opinions\nLinks to individual authors:\nKotia’s Home\nPosts"},"Kotia/Exploring-Process-Management---Linux-Kernel-Internals":{"slug":"Kotia/Exploring-Process-Management---Linux-Kernel-Internals","filePath":"Kotia/Exploring Process Management - Linux Kernel Internals.md","title":"Exploring Linux Kernel's Process Management","links":[],"tags":[],"content":"These are all the notes while i am learning the linux kernel’s internals specifically Process Management\nCurrently I am learning these topics will update as i learn more\nMy primary source is the Linux kernel development book\nCurrently this is the topic which i am learning\n\n\nProcess Definition\n\n\nProcess descriptor code\n\n\nfork(), exec() what do they do and how do they work?\n\n\nProcess Definition\nA process is a program (a.out code)in the midst of execution\nProcess also includes a set of resources such as open files and pending signals,\ninternal kernel data, processor state, a memory address space with one or more memory\nmappings, one or more threads of execution, and a data section containing global variables.\nProcesses, in effect, are the living result of running program code. The kernel needs to\nmanage all these details efficiently and transparently\nThreads of execution, often shortened to threads, are the objects of activity within the\nprocess.\nEach thread includes a unique program counter, process stack, and set of proces-\nsor registers.\nThe kernel schedules individual threads, not processes. In traditional Unix systems, each process consists of one thread.\nLinux has a unique implementation of threads: It does not differentiate between threads and processes.To Linux, a thread is just a special kind of process\nnote that threads share the virtual memory abstraction, whereas each\nreceives its own virtualized processor.\nA program itself is not a process; a process is an active program and related resources. Indeed, two or more processes can exist that are executing the same program. In fact, two or more processes can exist that share various resources, such as open files or an address space.\\\nThe process that calls fork() is the parent, whereas the new process is the child. The parent resumes execution and the child starts execution at the same place: where the call to fork() returns.The fork() system call returns from the kernel twice: once in the parent process and again in the newborn child.\nOften, immediately after a fork it is desirable to execute a new, different program.The exec() family of function calls creates a new address space and loads a new program into it. In contemporary Linux kernels, fork() is actually implemented via the clone() system call, which is discussed in a following section.\nA parent process can inquire about the status of a terminated child via the wait4() system call, which enables a process to wait for the termination of a specific process.When a process exits, it is placed into a special zombie state that represents terminated processes until the parent calls wait() or waitpid().\\\nNote→ Another name for a process is a task. The Linux kernel internally refers to processes as tasks.\nfork() workings\nNeed to explore this\nProcess Descriptor and Task Structure\nThe kernel stores the list of processes in a circular doubly linked list called the task list(some operating systems also call it as the task array)\nEach element in the task list is a process descriptor of the type struct task_struct, which\nis defined in &lt;linux/sched.h&gt;.The process descriptor contains all the information about\na specific process\nThe structure contains all the information that the kernel has and needs about a process.The process descriptor contains\n\n\n                  \n                  &lt;/include/linux/sched.h&gt; definition of task_struct \n                  \n                \n\nstruct task_struct {\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\t/*\n\t * For reasons of header soup (see current_thread_info()), this\n\t * must be the first element of task_struct.\n\t */\n\tstruct thread_info\t\tthread_info;\n#endif\n\tunsigned int\t\t\t__state;\n \n\t/* saved state for &quot;spinlock sleepers&quot; */\n\tunsigned int\t\t\tsaved_state;\n \n\t/*\n\t * This begins the randomizable portion of task_struct. Only\n\t * scheduling-critical items should be added above here.\n\t */\n\trandomized_struct_fields_start\n \n\tvoid\t\t\t\t*stack;\n\trefcount_t\t\t\tusage;\n\t/* Per task flags (PF_*), defined further below: */\n\tunsigned int\t\t\tflags;\n\tunsigned int\t\t\tptrace;\n \n#ifdef CONFIG_MEM_ALLOC_PROFILING\n\tstruct alloc_tag\t\t*alloc_tag;\n#endif\n \n\tint\t\t\t\ton_cpu;\n\tstruct __call_single_node\twake_entry;\n\tunsigned int\t\t\twakee_flips;\n\tunsigned long\t\t\twakee_flip_decay_ts;\n\tstruct task_struct\t\t*last_wakee;\n \n\t/*\n\t * recent_used_cpu is initially set as the last CPU used by a task\n\t * that wakes affine another task. Waker/wakee relationships can\n\t * push tasks around a CPU where each wakeup moves to the next one.\n\t * Tracking a recently used CPU allows a quick search for a recently\n\t * used CPU that may be idle.\n\t */\n\tint\t\t\t\trecent_used_cpu;\n\tint\t\t\t\twake_cpu;\n\tint\t\t\t\ton_rq;\n \n\tint\t\t\t\tprio;\n\tint\t\t\t\tstatic_prio;\n\tint\t\t\t\tnormal_prio;\n\tunsigned int\t\t\trt_priority;\n \n\tstruct sched_entity\t\tse;\n\tstruct sched_rt_entity\t\trt;\n\tstruct sched_dl_entity\t\tdl;\n\tstruct sched_dl_entity\t\t*dl_server;\n#ifdef CONFIG_SCHED_CLASS_EXT\n\tstruct sched_ext_entity\t\tscx;\n#endif\n\tconst struct sched_class\t*sched_class;\n \n#ifdef CONFIG_SCHED_CORE\n\tstruct rb_node\t\t\tcore_node;\n\tunsigned long\t\t\tcore_cookie;\n\tunsigned int\t\t\tcore_occupation;\n#endif\n \n#ifdef CONFIG_CGROUP_SCHED\n\tstruct task_group\t\t*sched_task_group;\n#endif\n \n \n#ifdef CONFIG_UCLAMP_TASK\n\t/*\n\t * Clamp values requested for a scheduling entity.\n\t * Must be updated with task_rq_lock() held.\n\t */\n\tstruct uclamp_se\t\tuclamp_req[UCLAMP_CNT];\n\t/*\n\t * Effective clamp values used for a scheduling entity.\n\t * Must be updated with task_rq_lock() held.\n\t */\n\tstruct uclamp_se\t\tuclamp[UCLAMP_CNT];\n#endif\n \n\tstruct sched_statistics         stats;\n \n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\t/* List of struct preempt_notifier: */\n\tstruct hlist_head\t\tpreempt_notifiers;\n#endif\n \n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tunsigned int\t\t\tbtrace_seq;\n#endif\n \n\tunsigned int\t\t\tpolicy;\n\tunsigned long\t\t\tmax_allowed_capacity;\n\tint\t\t\t\tnr_cpus_allowed;\n\tconst cpumask_t\t\t\t*cpus_ptr;\n\tcpumask_t\t\t\t*user_cpus_ptr;\n\tcpumask_t\t\t\tcpus_mask;\n\tvoid\t\t\t\t*migration_pending;\n\tunsigned short\t\t\tmigration_disabled;\n\tunsigned short\t\t\tmigration_flags;\n \n#ifdef CONFIG_PREEMPT_RCU\n\tint\t\t\t\trcu_read_lock_nesting;\n\tunion rcu_special\t\trcu_read_unlock_special;\n\tstruct list_head\t\trcu_node_entry;\n\tstruct rcu_node\t\t\t*rcu_blocked_node;\n#endif /* #ifdef CONFIG_PREEMPT_RCU */\n \n#ifdef CONFIG_TASKS_RCU\n\tunsigned long\t\t\trcu_tasks_nvcsw;\n\tu8\t\t\t\trcu_tasks_holdout;\n\tu8\t\t\t\trcu_tasks_idx;\n\tint\t\t\t\trcu_tasks_idle_cpu;\n\tstruct list_head\t\trcu_tasks_holdout_list;\n\tint\t\t\t\trcu_tasks_exit_cpu;\n\tstruct list_head\t\trcu_tasks_exit_list;\n#endif /* #ifdef CONFIG_TASKS_RCU */\n \n#ifdef CONFIG_TASKS_TRACE_RCU\n\tint\t\t\t\ttrc_reader_nesting;\n\tint\t\t\t\ttrc_ipi_to_cpu;\n\tunion rcu_special\t\ttrc_reader_special;\n\tstruct list_head\t\ttrc_holdout_list;\n\tstruct list_head\t\ttrc_blkd_node;\n\tint\t\t\t\ttrc_blkd_cpu;\n#endif /* #ifdef CONFIG_TASKS_TRACE_RCU */\n \n\tstruct sched_info\t\tsched_info;\n \n\tstruct list_head\t\ttasks;\n\tstruct plist_node\t\tpushable_tasks;\n\tstruct rb_node\t\t\tpushable_dl_tasks;\n \n\tstruct mm_struct\t\t*mm;\n\tstruct mm_struct\t\t*active_mm;\n\tstruct address_space\t\t*faults_disabled_mapping;\n \n\tint\t\t\t\texit_state;\n\tint\t\t\t\texit_code;\n\tint\t\t\t\texit_signal;\n\t/* The signal sent when the parent dies: */\n\tint\t\t\t\tpdeath_signal;\n\t/* JOBCTL_*, siglock protected: */\n\tunsigned long\t\t\tjobctl;\n \n\t/* Used for emulating ABI behavior of previous Linux versions: */\n\tunsigned int\t\t\tpersonality;\n \n\t/* Scheduler bits, serialized by scheduler locks: */\n\tunsigned\t\t\tsched_reset_on_fork:1;\n\tunsigned\t\t\tsched_contributes_to_load:1;\n\tunsigned\t\t\tsched_migrated:1;\n\tunsigned\t\t\tsched_task_hot:1;\n \n\t/* Force alignment to the next boundary: */\n\tunsigned\t\t\t:0;\n \n\t/* Unserialized, strictly &#039;current&#039; */\n \n\t/*\n\t * This field must not be in the scheduler word above due to wakelist\n\t * queueing no longer being serialized by p-&gt;on_cpu. However:\n\t *\n\t * p-&gt;XXX = X;\t\t\tttwu()\n\t * schedule()\t\t\t  if (p-&gt;on_rq &amp;&amp; ..) // false\n\t *   smp_mb__after_spinlock();\t  if (smp_load_acquire(&amp;p-&gt;on_cpu) &amp;&amp; //true\n\t *   deactivate_task()\t\t      ttwu_queue_wakelist())\n\t *     p-&gt;on_rq = 0;\t\t\tp-&gt;sched_remote_wakeup = Y;\n\t *\n\t * guarantees all stores of &#039;current&#039; are visible before\n\t * -&gt;sched_remote_wakeup gets used, so it can be in this word.\n\t */\n\tunsigned\t\t\tsched_remote_wakeup:1;\n#ifdef CONFIG_RT_MUTEXES\n\tunsigned\t\t\tsched_rt_mutex:1;\n#endif\n \n\t/* Bit to tell TOMOYO we&#039;re in execve(): */\n\tunsigned\t\t\tin_execve:1;\n\tunsigned\t\t\tin_iowait:1;\n#ifndef TIF_RESTORE_SIGMASK\n\tunsigned\t\t\trestore_sigmask:1;\n#endif\n#ifdef CONFIG_MEMCG_V1\n\tunsigned\t\t\tin_user_fault:1;\n#endif\n#ifdef CONFIG_LRU_GEN\n\t/* whether the LRU algorithm may apply to this access */\n\tunsigned\t\t\tin_lru_fault:1;\n#endif\n#ifdef CONFIG_COMPAT_BRK\n\tunsigned\t\t\tbrk_randomized:1;\n#endif\n#ifdef CONFIG_CGROUPS\n\t/* disallow userland-initiated cgroup migration */\n\tunsigned\t\t\tno_cgroup_migration:1;\n\t/* task is frozen/stopped (used by the cgroup freezer) */\n\tunsigned\t\t\tfrozen:1;\n#endif\n#ifdef CONFIG_BLK_CGROUP\n\tunsigned\t\t\tuse_memdelay:1;\n#endif\n#ifdef CONFIG_PSI\n\t/* Stalled due to lack of memory */\n\tunsigned\t\t\tin_memstall:1;\n#endif\n#ifdef CONFIG_PAGE_OWNER\n\t/* Used by page_owner=on to detect recursion in page tracking. */\n\tunsigned\t\t\tin_page_owner:1;\n#endif\n#ifdef CONFIG_EVENTFD\n\t/* Recursion prevention for eventfd_signal() */\n\tunsigned\t\t\tin_eventfd:1;\n#endif\n#ifdef CONFIG_ARCH_HAS_CPU_PASID\n\tunsigned\t\t\tpasid_activated:1;\n#endif\n#ifdef CONFIG_X86_BUS_LOCK_DETECT\n\tunsigned\t\t\treported_split_lock:1;\n#endif\n#ifdef CONFIG_TASK_DELAY_ACCT\n\t/* delay due to memory thrashing */\n\tunsigned                        in_thrashing:1;\n#endif\n\tunsigned\t\t\tin_nf_duplicate:1;\n#ifdef CONFIG_PREEMPT_RT\n\tstruct netdev_xmit\t\tnet_xmit;\n#endif\n\tunsigned long\t\t\tatomic_flags; /* Flags requiring atomic access. */\n \n\tstruct restart_block\t\trestart_block;\n \n\tpid_t\t\t\t\tpid;\n\tpid_t\t\t\t\ttgid;\n \n#ifdef CONFIG_STACKPROTECTOR\n\t/* Canary value for the -fstack-protector GCC feature: */\n\tunsigned long\t\t\tstack_canary;\n#endif\n\t/*\n\t * Pointers to the (original) parent process, youngest child, younger sibling,\n\t * older sibling, respectively.  (p-&gt;father can be replaced with\n\t * p-&gt;real_parent-&gt;pid)\n\t */\n \n\t/* Real parent process: */\n\tstruct task_struct __rcu\t*real_parent;\n \n\t/* Recipient of SIGCHLD, wait4() reports: */\n\tstruct task_struct __rcu\t*parent;\n \n\t/*\n\t * Children/sibling form the list of natural children:\n\t */\n\tstruct list_head\t\tchildren;\n\tstruct list_head\t\tsibling;\n\tstruct task_struct\t\t*group_leader;\n \n\t/*\n\t * &#039;ptraced&#039; is the list of tasks this task is using ptrace() on.\n\t *\n\t * This includes both natural children and PTRACE_ATTACH targets.\n\t * &#039;ptrace_entry&#039; is this task&#039;s link on the p-&gt;parent-&gt;ptraced list.\n\t */\n\tstruct list_head\t\tptraced;\n\tstruct list_head\t\tptrace_entry;\n \n\t/* PID/PID hash table linkage. */\n\tstruct pid\t\t\t*thread_pid;\n\tstruct hlist_node\t\tpid_links[PIDTYPE_MAX];\n\tstruct list_head\t\tthread_node;\n \n\tstruct completion\t\t*vfork_done;\n \n\t/* CLONE_CHILD_SETTID: */\n\tint __user\t\t\t*set_child_tid;\n \n\t/* CLONE_CHILD_CLEARTID: */\n\tint __user\t\t\t*clear_child_tid;\n \n\t/* PF_KTHREAD | PF_IO_WORKER */\n\tvoid\t\t\t\t*worker_private;\n \n\tu64\t\t\t\tutime;\n\tu64\t\t\t\tstime;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tu64\t\t\t\tutimescaled;\n\tu64\t\t\t\tstimescaled;\n#endif\n\tu64\t\t\t\tgtime;\n\tstruct prev_cputime\t\tprev_cputime;\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tstruct vtime\t\t\tvtime;\n#endif\n \n#ifdef CONFIG_NO_HZ_FULL\n\tatomic_t\t\t\ttick_dep_mask;\n#endif\n\t/* Context switch counts: */\n\tunsigned long\t\t\tnvcsw;\n\tunsigned long\t\t\tnivcsw;\n \n\t/* Monotonic time in nsecs: */\n\tu64\t\t\t\tstart_time;\n \n\t/* Boot based time in nsecs: */\n\tu64\t\t\t\tstart_boottime;\n \n\t/* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: */\n\tunsigned long\t\t\tmin_flt;\n\tunsigned long\t\t\tmaj_flt;\n \n\t/* Empty if CONFIG_POSIX_CPUTIMERS=n */\n\tstruct posix_cputimers\t\tposix_cputimers;\n \n#ifdef CONFIG_POSIX_CPU_TIMERS_TASK_WORK\n\tstruct posix_cputimers_work\tposix_cputimers_work;\n#endif\n \n\t/* Process credentials: */\n \n\t/* Tracer&#039;s credentials at attach: */\n\tconst struct cred __rcu\t\t*ptracer_cred;\n \n\t/* Objective and real subjective task credentials (COW): */\n\tconst struct cred __rcu\t\t*real_cred;\n \n\t/* Effective (overridable) subjective task credentials (COW): */\n\tconst struct cred __rcu\t\t*cred;\n \n#ifdef CONFIG_KEYS\n\t/* Cached requested key. */\n\tstruct key\t\t\t*cached_requested_key;\n#endif\n \n\t/*\n\t * executable name, excluding path.\n\t *\n\t * - normally initialized begin_new_exec()\n\t * - set it with set_task_comm()\n\t *   - strscpy_pad() to ensure it is always NUL-terminated and\n\t *     zero-padded\n\t *   - task_lock() to ensure the operation is atomic and the name is\n\t *     fully updated.\n\t */\n\tchar\t\t\t\tcomm[TASK_COMM_LEN];\n \n\tstruct nameidata\t\t*nameidata;\n \n#ifdef CONFIG_SYSVIPC\n\tstruct sysv_sem\t\t\tsysvsem;\n\tstruct sysv_shm\t\t\tsysvshm;\n#endif\n#ifdef CONFIG_DETECT_HUNG_TASK\n\tunsigned long\t\t\tlast_switch_count;\n\tunsigned long\t\t\tlast_switch_time;\n#endif\n\t/* Filesystem information: */\n\tstruct fs_struct\t\t*fs;\n \n\t/* Open file information: */\n\tstruct files_struct\t\t*files;\n \n#ifdef CONFIG_IO_URING\n\tstruct io_uring_task\t\t*io_uring;\n#endif\n \n\t/* Namespaces: */\n\tstruct nsproxy\t\t\t*nsproxy;\n \n\t/* Signal handlers: */\n\tstruct signal_struct\t\t*signal;\n\tstruct sighand_struct __rcu\t\t*sighand;\n\tsigset_t\t\t\tblocked;\n\tsigset_t\t\t\treal_blocked;\n\t/* Restored if set_restore_sigmask() was used: */\n\tsigset_t\t\t\tsaved_sigmask;\n\tstruct sigpending\t\tpending;\n\tunsigned long\t\t\tsas_ss_sp;\n\tsize_t\t\t\t\tsas_ss_size;\n\tunsigned int\t\t\tsas_ss_flags;\n \n\tstruct callback_head\t\t*task_works;\n \n#ifdef CONFIG_AUDIT\n#ifdef CONFIG_AUDITSYSCALL\n\tstruct audit_context\t\t*audit_context;\n#endif\n\tkuid_t\t\t\t\tloginuid;\n\tunsigned int\t\t\tsessionid;\n#endif\n\tstruct seccomp\t\t\tseccomp;\n\tstruct syscall_user_dispatch\tsyscall_dispatch;\n \n\t/* Thread group tracking: */\n\tu64\t\t\t\tparent_exec_id;\n\tu64\t\t\t\tself_exec_id;\n \n\t/* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */\n\tspinlock_t\t\t\talloc_lock;\n \n\t/* Protection of the PI data structures: */\n\traw_spinlock_t\t\t\tpi_lock;\n \n\tstruct wake_q_node\t\twake_q;\n \n#ifdef CONFIG_RT_MUTEXES\n\t/* PI waiters blocked on a rt_mutex held by this task: */\n\tstruct rb_root_cached\t\tpi_waiters;\n\t/* Updated under owner&#039;s pi_lock and rq lock */\n\tstruct task_struct\t\t*pi_top_task;\n\t/* Deadlock detection and priority inheritance handling: */\n\tstruct rt_mutex_waiter\t\t*pi_blocked_on;\n#endif\n \n\tstruct mutex\t\t\t*blocked_on;\t/* lock we&#039;re blocked on */\n \n#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER\n\t/*\n\t * Encoded lock address causing task block (lower 2 bits = type from\n\t * &lt;linux/hung_task.h&gt;). Accessed via hung_task_*() helpers.\n\t */\n\tunsigned long\t\t\tblocker;\n#endif\n \n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\tint\t\t\t\tnon_block_count;\n#endif\n \n#ifdef CONFIG_TRACE_IRQFLAGS\n\tstruct irqtrace_events\t\tirqtrace;\n\tunsigned int\t\t\thardirq_threaded;\n\tu64\t\t\t\thardirq_chain_key;\n\tint\t\t\t\tsoftirqs_enabled;\n\tint\t\t\t\tsoftirq_context;\n\tint\t\t\t\tirq_config;\n#endif\n#ifdef CONFIG_PREEMPT_RT\n\tint\t\t\t\tsoftirq_disable_cnt;\n#endif\n \n#ifdef CONFIG_LOCKDEP\n#define MAX_LOCK_DEPTH\t\t\t48UL\n\tu64\t\t\t\tcurr_chain_key;\n\tint\t\t\t\tlockdep_depth;\n\tunsigned int\t\t\tlockdep_recursion;\n\tstruct held_lock\t\theld_locks[MAX_LOCK_DEPTH];\n#endif\n \n#if defined(CONFIG_UBSAN) &amp;&amp; !defined(CONFIG_UBSAN_TRAP)\n\tunsigned int\t\t\tin_ubsan;\n#endif\n \n\t/* Journalling filesystem info: */\n\tvoid\t\t\t\t*journal_info;\n \n\t/* Stacked block device info: */\n\tstruct bio_list\t\t\t*bio_list;\n \n\t/* Stack plugging: */\n\tstruct blk_plug\t\t\t*plug;\n \n\t/* VM state: */\n\tstruct reclaim_state\t\t*reclaim_state;\n \n\tstruct io_context\t\t*io_context;\n \n#ifdef CONFIG_COMPACTION\n\tstruct capture_control\t\t*capture_control;\n#endif\n\t/* Ptrace state: */\n\tunsigned long\t\t\tptrace_message;\n\tkernel_siginfo_t\t\t*last_siginfo;\n \n\tstruct task_io_accounting\tioac;\n#ifdef CONFIG_PSI\n\t/* Pressure stall state */\n\tunsigned int\t\t\tpsi_flags;\n#endif\n#ifdef CONFIG_TASK_XACCT\n\t/* Accumulated RSS usage: */\n\tu64\t\t\t\tacct_rss_mem1;\n\t/* Accumulated virtual memory usage: */\n\tu64\t\t\t\tacct_vm_mem1;\n\t/* stime + utime since last update: */\n\tu64\t\t\t\tacct_timexpd;\n#endif\n#ifdef CONFIG_CPUSETS\n\t/* Protected by -&gt;alloc_lock: */\n\tnodemask_t\t\t\tmems_allowed;\n\t/* Sequence number to catch updates: */\n\tseqcount_spinlock_t\t\tmems_allowed_seq;\n\tint\t\t\t\tcpuset_mem_spread_rotor;\n#endif\n#ifdef CONFIG_CGROUPS\n\t/* Control Group info protected by css_set_lock: */\n\tstruct css_set __rcu\t\t*cgroups;\n\t/* cg_list protected by css_set_lock and tsk-&gt;alloc_lock: */\n\tstruct list_head\t\tcg_list;\n#endif\n#ifdef CONFIG_X86_CPU_RESCTRL\n\tu32\t\t\t\tclosid;\n\tu32\t\t\t\trmid;\n#endif\n#ifdef CONFIG_FUTEX\n\tstruct robust_list_head __user\t*robust_list;\n#ifdef CONFIG_COMPAT\n\tstruct compat_robust_list_head __user *compat_robust_list;\n#endif\n\tstruct list_head\t\tpi_state_list;\n\tstruct futex_pi_state\t\t*pi_state_cache;\n\tstruct mutex\t\t\tfutex_exit_mutex;\n\tunsigned int\t\t\tfutex_state;\n#endif\n#ifdef CONFIG_PERF_EVENTS\n\tu8\t\t\t\tperf_recursion[PERF_NR_CONTEXTS];\n\tstruct perf_event_context\t*perf_event_ctxp;\n\tstruct mutex\t\t\tperf_event_mutex;\n\tstruct list_head\t\tperf_event_list;\n\tstruct perf_ctx_data __rcu\t*perf_ctx_data;\n#endif\n#ifdef CONFIG_DEBUG_PREEMPT\n\tunsigned long\t\t\tpreempt_disable_ip;\n#endif\n#ifdef CONFIG_NUMA\n\t/* Protected by alloc_lock: */\n\tstruct mempolicy\t\t*mempolicy;\n\tshort\t\t\t\til_prev;\n\tu8\t\t\t\til_weight;\n\tshort\t\t\t\tpref_node_fork;\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\tint\t\t\t\tnuma_scan_seq;\n\tunsigned int\t\t\tnuma_scan_period;\n\tunsigned int\t\t\tnuma_scan_period_max;\n\tint\t\t\t\tnuma_preferred_nid;\n\tunsigned long\t\t\tnuma_migrate_retry;\n\t/* Migration stamp: */\n\tu64\t\t\t\tnode_stamp;\n\tu64\t\t\t\tlast_task_numa_placement;\n\tu64\t\t\t\tlast_sum_exec_runtime;\n\tstruct callback_head\t\tnuma_work;\n \n\t/*\n\t * This pointer is only modified for current in syscall and\n\t * pagefault context (and for tasks being destroyed), so it can be read\n\t * from any of the following contexts:\n\t *  - RCU read-side critical section\n\t *  - current-&gt;numa_group from everywhere\n\t *  - task&#039;s runqueue locked, task not running\n\t */\n\tstruct numa_group __rcu\t\t*numa_group;\n \n\t/*\n\t * numa_faults is an array split into four regions:\n\t * faults_memory, faults_cpu, faults_memory_buffer, faults_cpu_buffer\n\t * in this precise order.\n\t *\n\t * faults_memory: Exponential decaying average of faults on a per-node\n\t * basis. Scheduling placement decisions are made based on these\n\t * counts. The values remain static for the duration of a PTE scan.\n\t * faults_cpu: Track the nodes the process was running on when a NUMA\n\t * hinting fault was incurred.\n\t * faults_memory_buffer and faults_cpu_buffer: Record faults per node\n\t * during the current scan window. When the scan completes, the counts\n\t * in faults_memory and faults_cpu decay and these values are copied.\n\t */\n\tunsigned long\t\t\t*numa_faults;\n\tunsigned long\t\t\ttotal_numa_faults;\n \n\t/*\n\t * numa_faults_locality tracks if faults recorded during the last\n\t * scan window were remote/local or failed to migrate. The task scan\n\t * period is adapted based on the locality of the faults with different\n\t * weights depending on whether they were shared or private faults\n\t */\n\tunsigned long\t\t\tnuma_faults_locality[3];\n \n\tunsigned long\t\t\tnuma_pages_migrated;\n#endif /* CONFIG_NUMA_BALANCING */\n \n#ifdef CONFIG_RSEQ\n\tstruct rseq __user *rseq;\n\tu32 rseq_len;\n\tu32 rseq_sig;\n\t/*\n\t * RmW on rseq_event_mask must be performed atomically\n\t * with respect to preemption.\n\t */\n\tunsigned long rseq_event_mask;\n#ifdef CONFIG_DEBUG_RSEQ\n\t/*\n\t * This is a place holder to save a copy of the rseq fields for\n\t * validation of read-only fields. The struct rseq has a\n\t * variable-length array at the end, so it cannot be used\n\t * directly. Reserve a size large enough for the known fields.\n\t */\n\tchar\t\t\t\trseq_fields[sizeof(struct rseq)];\n#endif\n#endif\n \n#ifdef CONFIG_SCHED_MM_CID\n\tint\t\t\t\tmm_cid;\t\t/* Current cid in mm */\n\tint\t\t\t\tlast_mm_cid;\t/* Most recent cid in mm */\n\tint\t\t\t\tmigrate_from_cpu;\n\tint\t\t\t\tmm_cid_active;\t/* Whether cid bitmap is active */\n\tstruct callback_head\t\tcid_work;\n#endif\n \n\tstruct tlbflush_unmap_batch\ttlb_ubc;\n \n\t/* Cache last used pipe for splice(): */\n\tstruct pipe_inode_info\t\t*splice_pipe;\n \n\tstruct page_frag\t\ttask_frag;\n \n#ifdef CONFIG_TASK_DELAY_ACCT\n\tstruct task_delay_info\t\t*delays;\n#endif\n \n#ifdef CONFIG_FAULT_INJECTION\n\tint\t\t\t\tmake_it_fail;\n\tunsigned int\t\t\tfail_nth;\n#endif\n\t/*\n\t * When (nr_dirtied &gt;= nr_dirtied_pause), it&#039;s time to call\n\t * balance_dirty_pages() for a dirty throttling pause:\n\t */\n\tint\t\t\t\tnr_dirtied;\n\tint\t\t\t\tnr_dirtied_pause;\n\t/* Start of a write-and-pause period: */\n\tunsigned long\t\t\tdirty_paused_when;\n \n#ifdef CONFIG_LATENCYTOP\n\tint\t\t\t\tlatency_record_count;\n\tstruct latency_record\t\tlatency_record[LT_SAVECOUNT];\n#endif\n\t/*\n\t * Time slack values; these are used to round up poll() and\n\t * select() etc timeout values. These are in nanoseconds.\n\t */\n\tu64\t\t\t\ttimer_slack_ns;\n\tu64\t\t\t\tdefault_timer_slack_ns;\n \n#if defined(CONFIG_KASAN_GENERIC) || defined(CONFIG_KASAN_SW_TAGS)\n\tunsigned int\t\t\tkasan_depth;\n#endif\n \n#ifdef CONFIG_KCSAN\n\tstruct kcsan_ctx\t\tkcsan_ctx;\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tstruct irqtrace_events\t\tkcsan_save_irqtrace;\n#endif\n#ifdef CONFIG_KCSAN_WEAK_MEMORY\n\tint\t\t\t\tkcsan_stack_depth;\n#endif\n#endif\n \n#ifdef CONFIG_KMSAN\n\tstruct kmsan_ctx\t\tkmsan_ctx;\n#endif\n \n#if IS_ENABLED(CONFIG_KUNIT)\n\tstruct kunit\t\t\t*kunit_test;\n#endif\n \n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t/* Index of current stored address in ret_stack: */\n\tint\t\t\t\tcurr_ret_stack;\n\tint\t\t\t\tcurr_ret_depth;\n \n\t/* Stack of return addresses for return function tracing: */\n\tunsigned long\t\t\t*ret_stack;\n \n\t/* Timestamp for last schedule: */\n\tunsigned long long\t\tftrace_timestamp;\n\tunsigned long long\t\tftrace_sleeptime;\n \n\t/*\n\t * Number of functions that haven&#039;t been traced\n\t * because of depth overrun:\n\t */\n\tatomic_t\t\t\ttrace_overrun;\n \n\t/* Pause tracing: */\n\tatomic_t\t\t\ttracing_graph_pause;\n#endif\n \n#ifdef CONFIG_TRACING\n\t/* Bitmask and counter of trace recursion: */\n\tunsigned long\t\t\ttrace_recursion;\n#endif /* CONFIG_TRACING */\n \n#ifdef CONFIG_KCOV\n\t/* See kernel/kcov.c for more details. */\n \n\t/* Coverage collection mode enabled for this task (0 if disabled): */\n\tunsigned int\t\t\tkcov_mode;\n \n\t/* Size of the kcov_area: */\n\tunsigned int\t\t\tkcov_size;\n \n\t/* Buffer for coverage collection: */\n\tvoid\t\t\t\t*kcov_area;\n \n\t/* KCOV descriptor wired with this task or NULL: */\n\tstruct kcov\t\t\t*kcov;\n \n\t/* KCOV common handle for remote coverage collection: */\n\tu64\t\t\t\tkcov_handle;\n \n\t/* KCOV sequence number: */\n\tint\t\t\t\tkcov_sequence;\n \n\t/* Collect coverage from softirq context: */\n\tunsigned int\t\t\tkcov_softirq;\n#endif\n \n#ifdef CONFIG_MEMCG_V1\n\tstruct mem_cgroup\t\t*memcg_in_oom;\n#endif\n \n#ifdef CONFIG_MEMCG\n\t/* Number of pages to reclaim on returning to userland: */\n\tunsigned int\t\t\tmemcg_nr_pages_over_high;\n \n\t/* Used by memcontrol for targeted memcg charge: */\n\tstruct mem_cgroup\t\t*active_memcg;\n \n\t/* Cache for current-&gt;cgroups-&gt;memcg-&gt;objcg lookups: */\n\tstruct obj_cgroup\t\t*objcg;\n#endif\n \n#ifdef CONFIG_BLK_CGROUP\n\tstruct gendisk\t\t\t*throttle_disk;\n#endif\n \n#ifdef CONFIG_UPROBES\n\tstruct uprobe_task\t\t*utask;\n#endif\n#if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)\n\tunsigned int\t\t\tsequential_io;\n\tunsigned int\t\t\tsequential_io_avg;\n#endif\n\tstruct kmap_ctrl\t\tkmap_ctrl;\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\tunsigned long\t\t\ttask_state_change;\n#ifdef CONFIG_PREEMPT_RT\n\tunsigned long\t\t\tsaved_state_change;\n#endif\n#endif\n\tstruct rcu_head\t\t\trcu;\n\trefcount_t\t\t\trcu_users;\n\tint\t\t\t\tpagefault_disabled;\n#ifdef CONFIG_MMU\n\tstruct task_struct\t\t*oom_reaper_list;\n\tstruct timer_list\t\toom_reaper_timer;\n#endif\n#ifdef CONFIG_VMAP_STACK\n\tstruct vm_struct\t\t*stack_vm_area;\n#endif\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\t/* A live task holds one reference: */\n\trefcount_t\t\t\tstack_refcount;\n#endif\n#ifdef CONFIG_LIVEPATCH\n\tint patch_state;\n#endif\n#ifdef CONFIG_SECURITY\n\t/* Used by LSM modules for access restriction: */\n\tvoid\t\t\t\t*security;\n#endif\n#ifdef CONFIG_BPF_SYSCALL\n\t/* Used by BPF task local storage */\n\tstruct bpf_local_storage __rcu\t*bpf_storage;\n\t/* Used for BPF run context */\n\tstruct bpf_run_ctx\t\t*bpf_ctx;\n#endif\n\t/* Used by BPF for per-TASK xdp storage */\n\tstruct bpf_net_context\t\t*bpf_net_context;\n \n#ifdef CONFIG_KSTACK_ERASE\n\tunsigned long\t\t\tlowest_stack;\n#endif\n#ifdef CONFIG_KSTACK_ERASE_METRICS\n\tunsigned long\t\t\tprev_lowest_stack;\n#endif\n \n#ifdef CONFIG_X86_MCE\n\tvoid __user\t\t\t*mce_vaddr;\n\t__u64\t\t\t\tmce_kflags;\n\tu64\t\t\t\tmce_addr;\n\t__u64\t\t\t\tmce_ripv : 1,\n\t\t\t\t\tmce_whole_page : 1,\n\t\t\t\t\t__mce_reserved : 62;\n\tstruct callback_head\t\tmce_kill_me;\n\tint\t\t\t\tmce_count;\n#endif\n \n#ifdef CONFIG_KRETPROBES\n\tstruct llist_head               kretprobe_instances;\n#endif\n#ifdef CONFIG_RETHOOK\n\tstruct llist_head               rethooks;\n#endif\n \n#ifdef CONFIG_ARCH_HAS_PARANOID_L1D_FLUSH\n\t/*\n\t * If L1D flush is supported on mm context switch\n\t * then we use this callback head to queue kill work\n\t * to kill tasks that are not running on SMT disabled\n\t * cores\n\t */\n\tstruct callback_head\t\tl1d_flush_kill;\n#endif\n \n#ifdef CONFIG_RV\n\t/*\n\t * Per-task RV monitor, fixed in CONFIG_RV_PER_TASK_MONITORS.\n\t * If memory becomes a concern, we can think about a dynamic method.\n\t */\n\tunion rv_task_monitor\t\trv[CONFIG_RV_PER_TASK_MONITORS];\n#endif\n \n#ifdef CONFIG_USER_EVENTS\n\tstruct user_event_mm\t\t*user_event_mm;\n#endif\n \n#ifdef CONFIG_UNWIND_USER\n\tstruct unwind_task_info\t\tunwind_info;\n#endif\n \n\t/* CPU-specific state of this task: */\n\tstruct thread_struct\t\tthread;\n \n\t/*\n\t * New fields for task_struct should be added above here, so that\n\t * they are included in the randomized portion of task_struct.\n\t */\n\trandomized_struct_fields_end\n} __attribute__ ((aligned (64)));\n\n\ntestingint main(){\n\tpritnf(&quot;Hello, world!&quot;);\n}\nGoing into this behemoth of code isn’t something that we should do, once the time comes we will go into that i guess"},"Kotia/Kotia's-Home":{"slug":"Kotia/Kotia's-Home","filePath":"Kotia/Kotia's Home.md","title":"Kotia - Fish in the Sea","links":["Kotia/Exploring-Process-Management---Linux-Kernel-Internals"],"tags":[],"content":"This is Kotia’s homepage\nHere you will find my blogs or something else i haven’t decided it yet!\nPosts\nIn Progress\nI am treating these posts as more of my devlogs, rather than the definitive way to learn these topics, these are just my notes all in this page, don’t judge me on this.\nExploring Process Management - Linux Kernel Internals"}}